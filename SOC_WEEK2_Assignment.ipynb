{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPD75FlrXi3RvoIM5zUJfdm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunalr33/SOC_RlForAgents/blob/main/SOC_WEEK2_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "3GF5Em1cAPjO"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment setup\n",
        "grid_size = 4\n",
        "grid = [\n",
        "    [0,  1,  0,  10],\n",
        "    [0, -1,  0,   2],\n",
        "    [0,  0,  1,   0],\n",
        "    [-1,  0,  0,   5]\n",
        "]\n",
        "'''Action effects defined as position changes\n",
        "'U': Up -> (i-1, j)\n",
        "'D': Down -> (i+1, j)\n",
        "'L': Left -> (i, j-1)\n",
        "'R': Right -> (i, j+1)'''\n",
        "actions = ['U', 'D', 'L', 'R']\n",
        "action_effects = {'U': (-1, 0), 'D': (1, 0), 'L': (0, -1), 'R': (0, 1)}\n",
        "gamma = 0.95  # Discount factor\n",
        "theta = 0.0001  # Convergence threshold"
      ],
      "metadata": {
        "id": "71ZspPuCAWuR"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize value function and policy\n",
        "V = np.zeros((grid_size, grid_size)) #Vk for random policy\n",
        "policy = np.random.choice(actions, (grid_size, grid_size))"
      ],
      "metadata": {
        "id": "4ojG0dA5BqIv"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Evaluation\n",
        "def policy_evaluation(policy, V): # current value fn and policy as arg\n",
        "    while True:\n",
        "        delta = 0                 # delta track max change in V\n",
        "        for i in range(grid_size):    # Grid traversal\n",
        "            for j in range(grid_size):\n",
        "                v = V[i, j]\n",
        "                if grid[i][j] == -1:  # Skip obstacle cells\n",
        "                    continue\n",
        "                new_value = 0  # Initialize new value for (i,j) cell\n",
        "                action = policy[i, j]  # Current action for (i,j) cell by current policy\n",
        "                di, dj = action_effects[action] # change in co-ordinate\n",
        "                ni, nj = i + di, j + dj         #Next state\n",
        "                if 0 <= ni < grid_size and 0 <= nj < grid_size: # check if nextState within grid\n",
        "                    reward = grid[ni][nj]                       #R(t+1)\n",
        "                    new_value = reward + gamma * V[ni, nj]      #Bellman equation for update\n",
        "                V[i, j] = new_value\n",
        "                delta = max(delta, abs(v - V[i, j]))            #To check convergence\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V"
      ],
      "metadata": {
        "id": "-wJdfVgHESwl"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Improvement\n",
        "def policy_improvement(V):\n",
        "    policy_stable = True          # to check if policy remain same or not during improvemnet\n",
        "    for i in range(grid_size):    #grid traversal\n",
        "        for j in range(grid_size):\n",
        "            if grid[i][j] == -1:  # Skip obstacle cells\n",
        "                continue\n",
        "            old_action = policy[i, j] # current action according to V, stored to check if action changes\n",
        "            action_values = []        # empty list to store q\n",
        "            for action in actions:    # iteration for all possible action\n",
        "                di, dj = action_effects[action]\n",
        "                ni, nj = i + di, j + dj\n",
        "                if 0 <= ni < grid_size and 0 <= nj < grid_size:\n",
        "                    reward = grid[ni][nj]\n",
        "                    action_values.append((reward + gamma * V[ni, nj], action))\n",
        "            best_action = max(action_values, key=lambda x: x[0])[1] #determine best action\n",
        "            policy[i, j] = best_action                              # update policy\n",
        "            if old_action != best_action:                           # check if policy changes\n",
        "                policy_stable = False\n",
        "    return policy, policy_stable"
      ],
      "metadata": {
        "id": "3z3HLLaAGYHP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Iteration\n",
        "def policy_iteration():\n",
        "    global policy\n",
        "    global V\n",
        "    while True:\n",
        "        V = policy_evaluation(policy, V)                  #Returns updated valueFunction\n",
        "        policy, policy_stable = policy_improvement(V)     #Returns improved policy\n",
        "        if policy_stable:                                 #Check if policy optimal\n",
        "            break\n",
        "    return policy, V                                      #Returns optimal policy and valueFunction"
      ],
      "metadata": {
        "id": "p9zKx_uPKVoT"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the grid world\n",
        "def display_policy(policy):\n",
        "    for i in range(grid_size):\n",
        "        print(\" \".join(policy[i]))"
      ],
      "metadata": {
        "id": "PkmBPRZcKPR2"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the agent's path\n",
        "def visualize_path(policy, start):  #start: starting position in grid\n",
        "    path = []                       #sequence of positions the agent follow for optimalPolicy\n",
        "    i, j = start\n",
        "    while True:\n",
        "        path.append((i, j))\n",
        "        if grid[i][j] in {10, 5, 2, 1}:  # Stop if agent reaches a reward pot\n",
        "            break\n",
        "        action = policy[i, j]                  #action and change in position\n",
        "        di, dj = action_effects[action]\n",
        "        i, j = i + di, j + dj\n",
        "        if not (0 <= i < grid_size and 0 <= j < grid_size):  # Stop if agent goes out of grid\n",
        "            break\n",
        "    return path"
      ],
      "metadata": {
        "id": "00VyJidULxtr"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Policy Iteration\n",
        "optimal_policy, optimal_value_function = policy_iteration()"
      ],
      "metadata": {
        "id": "GvlTs6t_Ngv2"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Results\n",
        "print(\"Optimal Policy:\")\n",
        "display_policy(optimal_policy)\n",
        "print(\"Optimal Value Function:\")\n",
        "print(optimal_value_function)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ni1XEC3NwDX",
        "outputId": "66ef195e-36ee-4689-c97c-6081809740b5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "R R R D\n",
            "U L R U\n",
            "R R R U\n",
            "U R R U\n",
            "Optimal Value Function:\n",
            "[[111.15059895 115.948069   122.05066555 117.94813227]\n",
            " [105.593069     0.         117.94813227 122.05072566]\n",
            " [102.07566555 107.44813227 112.05072566 117.94818938]\n",
            " [  0.         105.87572566 111.44818938 112.05077991]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation\n",
        "start = (2, 0)  # Starting point\n",
        "path = visualize_path(optimal_policy, start)\n",
        "print(\"Path taken by the agent:\")\n",
        "print(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_xqq6DsN2cv",
        "outputId": "e1014cec-3c31-44e4-9d13-843449ecd8ca"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path taken by the agent:\n",
            "[(2, 0), (2, 1), (2, 2)]\n"
          ]
        }
      ]
    }
  ]
}