{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNycx2MS+PzYH7XRIOq1nq7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunalr33/SOC_RlForAgents/blob/main/Skiing_AtariGame_usingRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!apt-get install -y python3-opengl"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5WKEg2VdnfIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install gym[atari]"
      ],
      "metadata": {
        "id": "0opV5NeHrcZh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade gym"
      ],
      "metadata": {
        "id": "5TeCS8KL3mJr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autorom[accept-rom-license]"
      ],
      "metadata": {
        "id": "xioFj41LrbKQ",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa0be181-5cc9-433a-9171-e93626f87b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: autorom[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install atari_py"
      ],
      "metadata": {
        "collapsed": true,
        "id": "E56qlqOQuLdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25b401fa-68ec-4083-a150-8a9e933ee713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: atari_py in /usr/local/lib/python3.10/dist-packages (0.2.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from atari_py) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from atari_py) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FpEcy0AgOO5z",
        "outputId": "ebac2d09-75c0-4b81-a8db-caac839a3117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RefqAEDmlY3"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import random\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the DQN Model**"
      ],
      "metadata": {
        "id": "worQzeESKzo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dqn_model(input_shape, num_actions):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Convolutional layers\n",
        "    layer1 = layers.Conv2D(32, (8, 8), strides=(4, 4), activation='relu')(inputs)\n",
        "    layer2 = layers.Conv2D(64, (4, 4), strides=(2, 2), activation='relu')(layer1)\n",
        "    layer3 = layers.Conv2D(64, (3, 3), activation='relu')(layer2)\n",
        "\n",
        "    # Flatten and Dense layers\n",
        "    flattened = layers.Flatten()(layer3)\n",
        "    common = layers.Dense(512, activation='relu')(flattened)\n",
        "\n",
        "    # Dueling streams\n",
        "    value = layers.Dense(1)(common)\n",
        "    advantage = layers.Dense(num_actions)(common)\n",
        "\n",
        "    # Compute the mean advantage using a Lambda layer\n",
        "    mean_advantage = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1, keepdims=True))(advantage)\n",
        "\n",
        "    # Compute Q-values\n",
        "    q_values = value + (advantage - mean_advantage)\n",
        "\n",
        "    # Create and compile the model\n",
        "    model = tf.keras.Model(inputs, q_values)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "n6--PPODmrv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the Replay Buffer**"
      ],
      "metadata": {
        "id": "wHS2R4u1LE3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "JTeFjh8Umryz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the DQN Agent**"
      ],
      "metadata": {
        "id": "a-l6uCT2LONL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_shape, num_actions, replay_buffer_size=10000, batch_size=32, gamma=0.99, epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.999, target_update_freq=10000):\n",
        "        self.model = create_dqn_model(state_shape, num_actions)\n",
        "        self.target_model = create_dqn_model(state_shape, num_actions)\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "        self.replay_buffer = ReplayBuffer(replay_buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.num_actions = num_actions\n",
        "        self.target_update_freq = target_update_freq\n",
        "        self.steps = 0\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(self.num_actions)\n",
        "        q_values = self.model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def act(self, state):\n",
        "        return self.choose_action(state)\n",
        "\n",
        "    def learn(self):\n",
        "        if self.replay_buffer.size() < self.batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = self.replay_buffer.sample(self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "        states = np.array(states)\n",
        "        next_states = np.array(next_states)\n",
        "\n",
        "        targets = self.model.predict(states, verbose=0)\n",
        "        next_q_values = self.target_model.predict(next_states, verbose=0)\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            target = rewards[i]\n",
        "            if not dones[i]:\n",
        "                target += self.gamma * np.max(next_q_values[i])\n",
        "            targets[i, actions[i]] = target\n",
        "\n",
        "        loss = self.model.train_on_batch(states, targets)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        self.steps += 1\n",
        "        if self.steps % self.target_update_freq == 0:\n",
        "            self.update_target_model()\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "u4uAMSslmr1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Frame Preprocessing**"
      ],
      "metadata": {
        "id": "Tq7dFKUMLXwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_frame(frame):\n",
        "    frame = frame[35:195]  # Crop to play area\n",
        "    frame = cv2.resize(frame, (84, 84))  # Resize to 84x84\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
        "    frame = frame / 255.0  # Normalize pixel values\n",
        "    frame = np.expand_dims(frame, axis=-1)  # Add channel dimension\n",
        "    return frame"
      ],
      "metadata": {
        "id": "6ROq-OD-msIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the DQN Agent**"
      ],
      "metadata": {
        "id": "Yw1Z7ml9LdJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dqn(env_name, episodes=1000, max_steps_per_episode=200):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state_shape = (84, 84, 1)\n",
        "    num_actions = env.action_space.n\n",
        "    agent = DQNAgent(state_shape, num_actions)\n",
        "\n",
        "    episode_rewards = []\n",
        "    epsilon_values = []\n",
        "    losses = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        if isinstance(state, tuple):\n",
        "            state = state[0]\n",
        "        state = preprocess_frame(state)\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            action = agent.choose_action(state)\n",
        "            next_frame, reward, done, truncated, info = env.step(action)\n",
        "            next_state = preprocess_frame(next_frame)\n",
        "            agent.replay_buffer.add((state, action, reward, next_state, done))\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if agent.replay_buffer.size() >= agent.batch_size:\n",
        "                loss = agent.learn()\n",
        "                if loss is not None:\n",
        "                    losses.append(loss)\n",
        "\n",
        "            if done or truncated:\n",
        "                break\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "        epsilon_values.append(agent.epsilon)\n",
        "\n",
        "        print(f\"Episode {episode+1}/{episodes}, Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
        "\n",
        "    env.close()\n",
        "    return agent, episode_rewards, epsilon_values, losses\n"
      ],
      "metadata": {
        "id": "8rAQlSGkmsLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rewards**"
      ],
      "metadata": {
        "id": "lGx8gSRSLhxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_rewards(episode_rewards):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(episode_rewards, label='Episode Reward', color='blue')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.title('Agent Rewards Over Episodes')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VCqF-viS7ZJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metrics**"
      ],
      "metadata": {
        "id": "LmB6EX5zLpZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(episode_rewards, epsilon_values, losses=None):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(episode_rewards, label='Episode Reward', color='blue')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.title('Training Convergence - Rewards')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(epsilon_values, label='Epsilon', color='orange')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Epsilon')\n",
        "    plt.title('Epsilon Decay')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    if losses:\n",
        "        plt.subplot(3, 1, 3)\n",
        "        plt.plot(losses, label='Loss', color='red')\n",
        "        plt.xlabel('Update Steps')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "dZSf91QEEvk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Plot Results**"
      ],
      "metadata": {
        "id": "k0WcXoFHLvV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the agent and get rewards\n",
        "trained_agent, rewards, epsilon_values, losses = train_dqn('ALE/Skiing-v5', episodes=1000)\n",
        "\n",
        "# Plot the rewards\n",
        "plot_rewards(rewards)\n",
        "\n",
        "# Plot the metrics\n",
        "plot_metrics(rewards, epsilon_values, losses)"
      ],
      "metadata": {
        "id": "zIj5ObcXm_gk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "2279c6c8-64ec-4a4f-ad82-a5f723f46c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7a28dff7f9a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7a28dff7f9a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/1000, Reward: -1331.0, Epsilon: 0.8444374977929298\n",
            "Episode 2/1000, Reward: -1331.0, Epsilon: 0.6912977691360495\n",
            "Episode 3/1000, Reward: -1331.0, Epsilon: 0.5659301095244186\n",
            "Episode 4/1000, Reward: -1331.0, Epsilon: 0.46329802172888124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Frame Display Function**"
      ],
      "metadata": {
        "id": "3VS4kTcLMS8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_frame(frame):\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.imshow(frame)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    clear_output(wait=True)"
      ],
      "metadata": {
        "id": "wQyvNN3rm_j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image Preprocessing Function**"
      ],
      "metadata": {
        "id": "IMf6K-T_MdA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image):\n",
        "    # Convert to grayscale and resize\n",
        "    transform = T.Compose([\n",
        "        T.ToPILImage(),\n",
        "        T.Grayscale(),\n",
        "        T.Resize((84, 84)),\n",
        "        T.ToTensor()\n",
        "    ])\n",
        "    return transform(image).unsqueeze(0)  # Add batch dimension"
      ],
      "metadata": {
        "id": "1U4HaqkZojDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization Function**"
      ],
      "metadata": {
        "id": "s9naS2Z9MmZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import atari_py\n"
      ],
      "metadata": {
        "id": "FO9SzdRupT7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_agent(env_name, agent, num_frames=100):\n",
        "    # Create the environment with a render mode\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, info = env.reset()\n",
        "    done = False\n",
        "\n",
        "    for _ in range(num_frames):\n",
        "        frame = env.render()  # Capture frame from environment\n",
        "        show_frame(frame)  # Display the frame\n",
        "\n",
        "        # Get action from agent and step through environment\n",
        "        action = agent.act(state)\n",
        "        state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if done:\n",
        "            state, info = env.reset()\n",
        "\n",
        "    env.close()\n"
      ],
      "metadata": {
        "id": "rBuFKLvrMrPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualize Agent**"
      ],
      "metadata": {
        "id": "VjZU5czIMxgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example state_shape and num_actions\n",
        "state_shape = (84, 84, 1)\n",
        "num_actions = 3  # Update this based on your environment\n",
        "\n",
        "# Instantiate agent\n",
        "agent = DQNAgent(state_shape=state_shape, num_actions=num_actions)\n",
        "\n",
        "# Visualize the agent\n",
        "visualize_agent('ALE/Skiing-v5', agent, num_frames=100)"
      ],
      "metadata": {
        "id": "WiMAI7n5Mxue"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}